---
version: 0.2

branches:
  feature/*, documentation/*, improvement/*, bugfix/*, w/*, q/*, hotfix/*, dependabot/*, user/*:
    stage: pre-merge

models:
  - env: &global-env
      azurebackend_AZURE_STORAGE_ACCESS_KEY: >-
        %(secret:azure_storage_access_key)s
      azurebackend_AZURE_STORAGE_ACCOUNT_NAME: >-
        %(secret:azure_storage_account_name)s
      azurebackend_AZURE_STORAGE_ENDPOINT: >-
        %(secret:azure_storage_endpoint)s
      azurebackend2_AZURE_STORAGE_ACCESS_KEY: >-
        %(secret:azure_storage_access_key_2)s
      azurebackend2_AZURE_STORAGE_ACCOUNT_NAME: >-
        %(secret:azure_storage_account_name_2)s
      azurebackend2_AZURE_STORAGE_ENDPOINT: >-
        %(secret:azure_storage_endpoint_2)s
      azurebackendmismatch_AZURE_STORAGE_ACCESS_KEY: >-
        %(secret:azure_storage_access_key)s
      azurebackendmismatch_AZURE_STORAGE_ACCOUNT_NAME: >-
        %(secret:azure_storage_account_name)s
      azurebackendmismatch_AZURE_STORAGE_ENDPOINT: >-
        %(secret:azure_storage_endpoint)s
      azurenonexistcontainer_AZURE_STORAGE_ACCESS_KEY: >-
        %(secret:azure_storage_access_key)s
      azurenonexistcontainer_AZURE_STORAGE_ACCOUNT_NAME: >-
        %(secret:azure_storage_account_name)s
      azurenonexistcontainer_AZURE_STORAGE_ENDPOINT: >-
        %(secret:azure_storage_endpoint)s
      azuretest_AZURE_BLOB_ENDPOINT: "%(secret:azure_storage_endpoint)s"
      b2backend_B2_ACCOUNT_ID: "%(secret:b2backend_b2_account_id)s"
      b2backend_B2_STORAGE_ACCESS_KEY: >-
        %(secret:b2backend_b2_storage_access_key)s
      GOOGLE_SERVICE_EMAIL: "%(secret:gcp_service_email)s"
      GOOGLE_SERVICE_KEY: "%(secret:gcp_service_key)s"
      AWS_S3_BACKEND_ACCESS_KEY: "%(secret:aws_s3_backend_access_key)s"
      AWS_S3_BACKEND_SECRET_KEY: "%(secret:aws_s3_backend_secret_key)s"
      AWS_S3_BACKEND_ACCESS_KEY_2: "%(secret:aws_s3_backend_access_key_2)s"
      AWS_S3_BACKEND_SECRET_KEY_2: "%(secret:aws_s3_backend_secret_key_2)s"
      AWS_GCP_BACKEND_ACCESS_KEY: "%(secret:aws_gcp_backend_access_key)s"
      AWS_GCP_BACKEND_SECRET_KEY: "%(secret:aws_gcp_backend_secret_key)s"
      AWS_GCP_BACKEND_ACCESS_KEY_2: "%(secret:aws_gcp_backend_access_key_2)s"
      AWS_GCP_BACKEND_SECRET_KEY_2: "%(secret:aws_gcp_backend_secret_key_2)s"
      b2backend_B2_STORAGE_ENDPOINT: "%(secret:b2backend_b2_storage_endpoint)s"
      gcpbackend2_GCP_SERVICE_EMAIL: "%(secret:gcp2_service_email)s"
      gcpbackend2_GCP_SERVICE_KEY: "%(secret:gcp2_service_key)s"
      gcpbackend2_GCP_SERVICE_KEYFILE: /root/.gcp/servicekey
      gcpbackend_GCP_SERVICE_EMAIL: "%(secret:gcp_service_email)s"
      gcpbackend_GCP_SERVICE_KEY: "%(secret:gcp_service_key)s"
      gcpbackendmismatch_GCP_SERVICE_EMAIL: >-
        %(secret:gcpbackendmismatch_gcp_service_email)s
      gcpbackendmismatch_GCP_SERVICE_KEY: >-
        %(secret:gcpbackendmismatch_gcp_service_key)s
      gcpbackend_GCP_SERVICE_KEYFILE: /root/.gcp/servicekey
      gcpbackendmismatch_GCP_SERVICE_KEYFILE: /root/.gcp/servicekey
      gcpbackendnoproxy_GCP_SERVICE_KEYFILE: /root/.gcp/servicekey
      gcpbackendproxy_GCP_SERVICE_KEYFILE: /root/.gcp/servicekey
  - env: &mongo-vars
      S3BACKEND: "mem"
      MPU_TESTING: "yes"
      S3METADATA: mongodb
      S3KMS: "file"
  - env: &multiple-backend-vars
      S3BACKEND: "mem"
      S3DATA: "multiple"
      MPU_TESTING: "yes"
      S3KMS: "file"
  - env: &file-mem-mpu
      S3BACKEND: "file"
      S3VAULT: "mem"
      MPU_TESTING: "yes"
  - env: &oras
      REGISTRY: 'registry.scality.com'
      PROJECT: '%(prop:git_slug)s'
      LAYERS: >-
        dashboard.json:application/grafana-dashboard+json
        alerts.yaml:application/prometheus-alerts+yaml
  - Git: &clone
      name: Pull repo
      repourl: '%(prop:git_reference)s'
      shallow: true
      retryFetch: true
      haltOnFailure: true
  - ShellCommand: &credentials
      name: Setup Credentials
      command: bash eve/workers/build/credentials.bash
      haltOnFailure: true
      env: *global-env
  - ShellCommand: &yarn-install
      name: install modules
      command: yarn install --ignore-engines --frozen-lockfile
      haltOnFailure: true
  - ShellCommand: &check-s3-action-logs
      name: Check s3 action logs
      command: |
        LOGS=`cat /artifacts/s3.log | grep 'No actionLog'`
        test `echo -n ${LOGS} | wc -l` -eq 0 || (echo $LOGS && false)
  - Upload: &upload-artifacts
      source: /artifacts
      urls:
        - "*"
  - ShellCommand: &follow-s3-log
      logfiles:
        s3:
          filename: /artifacts/s3.log
          follow: true
  - ShellCommand: &follow-s3-ceph-logs
      logfiles:
        ceph:
          filename: /artifacts/ceph.log
          follow: true
        s3:
          filename: /artifacts/s3.log
          follow: true
  - ShellCommand: &add-hostname
      name: add hostname
      command: |
        echo "127.0.0.1 testrequestbucket.localhost" >> /etc/hosts
        echo \
        "127.0.0.1 bucketwebsitetester.s3-website-us-east-1.amazonaws.com" \
        >> /etc/hosts
      haltOnFailure: true
  - ShellCommand: &setup-junit-upload
      name: preparing junit files for upload
      command: |
        mkdir -p artifacts/junit
        find . -name "*junit*.xml" -exec cp {} artifacts/junit/ ";"
      alwaysRun: true
  - Upload: &upload-junits
      source: artifacts
      urls:
        - "*"
      alwaysRun: true
  - env: &docker_env
      DEVELOPMENT_DOCKER_IMAGE_NAME: >-
        registry.scality.com/%(prop:git_slug)s-dev/%(prop:git_slug)s
      PRODUCTION_DOCKER_IMAGE_NAME: >-
        registry.scality.com/%(prop:git_slug)s/%(prop:git_slug)s
  - ShellCommand: &docker_login
      name: Login to docker registry
      command: >
        docker login
        -u "${HARBOR_LOGIN}"
        -p "${HARBOR_PASSWORD}"
        registry.scality.com
      usePTY: true
      env:
        HARBOR_LOGIN: '%(secret:harbor_login)s'
        HARBOR_PASSWORD: '%(secret:harbor_password)s'
  - ShellCommand: &wait_docker_daemon
      name: Wait for Docker daemon to be ready
      command: |
        bash -c '
        for i in {1..150}
        do
          docker info &> /dev/null && exit
          sleep 2
        done
        echo "Could not reach Docker daemon from buildbot worker" >&2
        exit 1'
      haltOnFailure: true


stages:
  pre-merge:
    worker:
      type: local
    steps:
      - TriggerStages:
          name: Launch all workers
          stage_names:
            - docker-build
            - linting-coverage
            - file-ft-tests
            - multiple-backend-test
            - mongo-ft-tests
            - ceph-backend-tests
            - kmip-ft-tests
            - utapi-v2-tests
          waitForFinish: true
          haltOnFailure: true

  linting-coverage:
    worker:
      type: docker
      path: eve/workers/build
      volumes: &default_volumes
        - '/home/eve/workspace'
    steps:
      - Git: *clone
      - ShellCommand: *yarn-install
      - ShellCommand: *add-hostname
      - ShellCommand: *credentials
      - ShellCommand:
          name: Unit Coverage mandatory file
          command: |
            set -ex
            test -f .git/HEAD
      - ShellCommand:
          name: Linting
          command: |
            set -ex
            yarn run --silent lint -- --max-warnings 0
            yarn run --silent lint_md
            flake8 $(git ls-files "*.py")
            yamllint -c yamllint.yml $(git ls-files "*.yml")
      - ShellCommand:
          name: Unit Coverage
          command: |
            set -ex
            unset HTTP_PROXY HTTPS_PROXY NO_PROXY
            unset http_proxy https_proxy no_proxy
            mkdir -p $CIRCLE_TEST_REPORTS/unit
            yarn test
            yarn run test_versionid_base62
            yarn run test_legacy_location
          env: &shared-vars
            <<: *global-env
            S3_LOCATION_FILE: tests/locationConfig/locationConfigTests.json
            CIRCLE_TEST_REPORTS: /tmp
            CIRCLE_ARTIFACTS: /tmp
            CI_REPORTS: /tmp
      - ShellCommand:
          name: Unit Coverage logs
          command: find /tmp/unit -exec cat {} \;
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-junits

  multiple-backend-test:
    worker:
      type: kube_pod
      path: eve/workers/pod.yaml
      images:
        aggressor: eve/workers/build
        s3: "."
      vars:
        aggressorMem: "2560Mi"
        s3Mem: "2560Mi"
        env:
          <<: *multiple-backend-vars
          <<: *global-env
    steps:
      - Git: *clone
      - ShellCommand: *credentials
      - ShellCommand: *yarn-install
      - ShellCommand:
          command: |
            bash -c "
            source /root/.aws/exports &> /dev/null
            set -ex
            bash wait_for_local_port.bash 8000 40
            yarn run multiple_backend_test
            yarn run ft_awssdk_external_backends"
          <<: *follow-s3-log
          env:
            <<: *multiple-backend-vars
            <<: *global-env
            S3_LOCATION_FILE: tests/locationConfig/locationConfigTests.json
      - ShellCommand:
          command: mvn test
          workdir: build/tests/functional/jaws
          <<: *follow-s3-log
          env:
            <<: *multiple-backend-vars
      - ShellCommand:
          command: rspec tests.rb
          workdir: build/tests/functional/fog
          <<: *follow-s3-log
          env:
            <<: *multiple-backend-vars
      - ShellCommand: *check-s3-action-logs
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-artifacts
      - Upload: *upload-junits

  ceph-backend-tests:
    worker:
      type: kube_pod
      path: eve/workers/pod.yaml
      images:
        aggressor: eve/workers/build
        s3: "."
        ceph: eve/workers/ceph
      vars:
        aggressorMem: "2500Mi"
        s3Mem: "2560Mi"
        redis: enabled
        env:
          <<: *multiple-backend-vars
          <<: *global-env
          S3METADATA: mongodb
          CI_CEPH: "true"
          MPU_TESTING: "yes"
          S3_LOCATION_FILE: tests/locationConfig/locationConfigCeph.json
    steps:
      - Git: *clone
      - ShellCommand: *credentials
      - ShellCommand: *yarn-install
      - ShellCommand:
          command: |
            bash -c "
            source /root/.aws/exports &> /dev/null
            set -ex
            bash eve/workers/ceph/wait_for_ceph.sh
            bash wait_for_local_port.bash 27018 40
            bash wait_for_local_port.bash 8000 40
            yarn run multiple_backend_test"
          env:
            <<: *multiple-backend-vars
            <<: *global-env
            S3METADATA: mem
          <<: *follow-s3-ceph-logs
      - ShellCommand:
          command: mvn test
          workdir: build/tests/functional/jaws
          <<: *follow-s3-ceph-logs
          env:
            <<: *multiple-backend-vars
      - ShellCommand:
          command: rspec tests.rb
          workdir: build/tests/functional/fog
          <<: *follow-s3-ceph-logs
          env:
            <<: *multiple-backend-vars
      - ShellCommand:
          command: |
            yarn run ft_awssdk &&
            yarn run ft_s3cmd
          env:
            <<: *file-mem-mpu
            <<: *global-env
            S3METADATA: mongodb
            S3_LOCATION_FILE: "/kube_pod-prod-cloudserver-backend-0/\
              build/tests/locationConfig/locationConfigCeph.json"
          <<: *follow-s3-ceph-logs
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-artifacts
      - Upload: *upload-junits

  mongo-ft-tests:
    worker: &s3-pod
      type: kube_pod
      path: eve/workers/pod.yaml
      images:
        aggressor: eve/workers/build
        s3: "."
      vars:
        aggressorMem: "2Gi"
        s3Mem: "1664Mi"
        redis: enabled
        env:
          <<: *mongo-vars
          <<: *global-env
    steps:
      - Git: *clone
      - ShellCommand: *credentials
      - ShellCommand: *yarn-install
      - ShellCommand:
          command: |
            set -ex
            bash wait_for_local_port.bash 8000 40
            yarn run ft_test
          <<: *follow-s3-log
          env:
            <<: *mongo-vars
            <<: *global-env
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-artifacts
      - Upload: *upload-junits

  file-ft-tests:
    worker:
      type: kube_pod
      path: eve/workers/pod.yaml
      images:
        aggressor: eve/workers/build
        s3: "."
      vars:
        aggressorMem: "3Gi"
        s3Mem: "2560Mi"
        redis: enabled
        env:
          <<: *file-mem-mpu
          <<: *global-env
    steps:
      - Git: *clone
      - ShellCommand: *credentials
      - ShellCommand: *yarn-install
      - ShellCommand:
          command: |
            set -ex
            bash wait_for_local_port.bash 8000 40
            yarn run ft_test
          <<: *follow-s3-log
          env:
            <<: *file-mem-mpu
            <<: *global-env
      - ShellCommand: *check-s3-action-logs
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-artifacts
      - Upload: *upload-junits

  kmip-ft-tests:
    worker:
      type: kube_pod
      path: eve/workers/pod.yaml
      images:
        aggressor: eve/workers/build
        s3: "."
        pykmip: eve/workers/pykmip
      vars:
        aggressorMem: "2Gi"
        s3Mem: "1664Mi"
        redis: enabled
        pykmip: enabled
        env:
          <<: *file-mem-mpu
          <<: *global-env
    steps:
      - Git: *clone
      - ShellCommand: *credentials
      - ShellCommand: *yarn-install
      - ShellCommand:
          command: |
            set -ex
            bash wait_for_local_port.bash 8000 40
            bash wait_for_local_port.bash 5696 40
            yarn run ft_kmip
          logfiles:
            pykmip:
              filename: /artifacts/pykmip.log
              follow: true
            s3:
              filename: /artifacts/s3.log
              follow: true
          env:
            <<: *file-mem-mpu
            <<: *global-env
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-artifacts
      - Upload: *upload-junits

  utapi-v2-tests:
    worker:
      type: kube_pod
      path: eve/workers/pod.yaml
      images:
        aggressor: eve/workers/build
        s3: "."
      vars:
        aggressorMem: "2Gi"
        s3Mem: "2Gi"
        env:
          ENABLE_UTAPI_V2: t
          S3BACKEND: mem
          BUCKET_DENY_FILTER: utapi-event-filter-deny-bucket
    steps:
      - Git: *clone
      - ShellCommand: *credentials
      - ShellCommand: *yarn-install
      - ShellCommand:
          command: |
            bash -c "
            source /root/.aws/exports &> /dev/null
            set -ex
            bash wait_for_local_port.bash 8000 40
            yarn run test_utapi_v2"
          <<: *follow-s3-log
          env:
            ENABLE_UTAPI_V2: t
            S3BACKEND: mem
      - ShellCommand: *check-s3-action-logs
      - ShellCommand: *setup-junit-upload
      - Upload: *upload-artifacts
      - Upload: *upload-junits

  # The docker-build stage ensures that your images are built on every commit
  # and also hosted on the registry to help you pull it up and
  # test it in a real environment if needed.
  # It also allows us to pull and rename it when performing a release.
  docker-build:
    worker: &docker_worker
      type: kube_pod
      path: eve/workers/docker/pod.yaml
      images:
        worker: eve/workers/docker
    steps:
      - Git: *clone
      - ShellCommand: *wait_docker_daemon
      - ShellCommand: *docker_login
      - ShellCommand:
          name: docker build
          command: >-
            docker build .
            --tag=${DEVELOPMENT_DOCKER_IMAGE_NAME}:%(prop:commit_short_revision)s
          env: *docker_env
          haltOnFailure: true
      - ShellCommand:
          name: push docker image into the development namespace
          command: docker push ${DEVELOPMENT_DOCKER_IMAGE_NAME}
          haltOnFailure: true
          env: *docker_env
      - ShellCommand: &oras_login
          name: Oras login
          command:
            oras login --username "${HARBOR_LOGIN}" --password "${HARBOR_PASSWORD}" ${REGISTRY}
          env:
            <<: *oras
            HARBOR_LOGIN: '%(secret:harbor_login)s'
            HARBOR_PASSWORD: '%(secret:harbor_password)s'
      - ShellCommand:
          name: push dashboards to the development namespace
          command: |
            for revision in %(prop:commit_short_revision)s latest ; do
              oras push ${REGISTRY}/${PROJECT}-dev/${PROJECT}-dashboards:$revision ${LAYERS}
            done
          env: *oras
          workdir: build/monitoring/


  # This stage can be used to release your Docker image.
  # To use this stage:
  # 1. Tag the repository
  # 2. Force a build using:
  #    * A branch that ideally matches the tag
  #    * The release stage
  #    * An extra property with the name tag and its value being the actual tag
  release:
    worker:
      type: local
    steps:
      - TriggerStages:
          stage_names:
            - docker-release
          haltOnFailure: true
  docker-release:
    worker: *docker_worker
    steps:
      - Git: *clone
      - ShellCommand: *wait_docker_daemon
      - ShellCommand: *docker_login
      - ShellCommand:
          name: Checkout tag
          command: git checkout refs/tags/%(prop:tag)s
          haltOnFailure: true
      - ShellCommand:
          name: docker build
          command: >-
            docker build .
            --tag=${PRODUCTION_DOCKER_IMAGE_NAME}:%(prop:tag)s
          env: *docker_env
      - ShellCommand:
          name: publish docker image to Scality Production OCI registry
          command: docker push ${PRODUCTION_DOCKER_IMAGE_NAME}:%(prop:tag)s
          env: *docker_env
      - ShellCommand: *oras_login
      - ShellCommand:
          name: push dashboards to the production namespace
          command: |
            oras push ${REGISTRY}/${PROJECT}/${PROJECT}-dashboards:%(prop:tag)s ${LAYERS}
          env: *oras
          workdir: build/monitoring/
